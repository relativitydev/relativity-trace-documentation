# Relativity Trace User Documentation

- [Release Notes](#release-notes)
- [Introduction to Relativity Trace](#introduction-to-relativity-trace)
- [Prerequisites](#prerequisites)
	- [Agents](#agents)
	- [Applications](#applications)
	- [BIST ( Built-in self-test ) Workspace](#bist--built-in-self-test--workspace)
- [Setting up Relativity Trace](#setting-up-relativity-trace)
- [Trace Document Flow Overview](#trace-document-flow-overview)
	- [Trace Document Fields](#trace-document-fields)
	- [Dashboard Widgets](#dashboard-widgets)
	- [Trace Document Retry and Error Resolution Workflow](#trace-document-retry-and-error-resolution-workflow)
- [Trace Rules Engine Overview](#trace-rules-engine-overview)
	- [1 - Creating a Rule](#1---creating-a-rule)
	- [2 - Customizing and Running a Rule](#2---customizing-and-running-a-rule)
	- [3 - Validating Results](#3---validating-results)
	- [Terms](#terms)
		- [Creating Terms](#creating-terms)
		- [Highlighting](#highlighting)
	- [Actions](#actions)
		- [Move To Folder Action Type](#move-to-folder-action-type)
		- [Data Disposal Action Type](#data-disposal-action-type)
		- [Advanced Action Type](#advanced-action-type)
		- [Alert Action Types](#alert-action-types)
		- [Replacement Tokens](#replacement-tokens)
			- [Email Action Type](#email-action-type)
				- [Sample Email generated by Trace](#sample-email-generated-by-trace)
			- [Slack Action Type](#slack-action-type)
				- [Sample Slack message generated by Trace](#sample-slack-message-generated-by-trace)
			- [Webhook Action Type (Preview)](#webhook-action-type-preview)
		- [Custom Relativity Scripts](#custom-relativity-scripts)
- [Trace Proactive Ingestion Framework](#trace-proactive-ingestion-framework)
	- [Data Sources](#data-sources)
    
		    - [Data Source Specific Settings](#data-source-specific-settings)
		
		- [Microsoft Exchange Data Source](#microsoft-exchange-data-source)
		- [Relativity Native Data Extraction Data Source](#relativity-native-data-extraction-data-source)
	- [Monitored Individuals](#monitored-individuals)
	- [Data Transformations](#data-transformations)
		- [Replace Data Transformation](#replace-data-transformation)
		- [Deduplication Data Transformation](#deduplication-data-transformation)
		  - [Required Fields for Deduplication](#required-fields-for-deduplication)
	- [Data Batches](#data-batches)
	  
	  - [Data Batch Retry and Error Resolution Workflow](#data-batch-retry-and-error-resolution-workflow)
	- [Discovery of Monitored Individuals](#discovery-of-monitored-individuals)
	  - [Monitored Individual Discovery On Globanet Data Sources](#monitored-individual-discovery-on-globanet-data-sources)
	  - [Monitored Individual Discovery On Other Data Sources](#monitored-individual-discovery-on-other-data-sources)
	  - [Supported File Formats](#supported-file-formats)
- [Setup](#setup)
	- [Tasks](#tasks)
	- [Alerts and Notifications](#alerts-and-notifications)
	- [Errors and Logging](#errors-and-logging)
- [Analytics Automation](#analytics-automation)
  - [Conceptual and Classification Analytics](#conceptual-and-classification-analytics)
  - [Structured Analytics Sets](#structured-analytics-sets)
- [Built-In Self-Test (BIST)](#built-in-self-test-bist)
- [Reporting](#reporting)
	
	- [Trace Terms Report](#trace-terms-report)
- [Trace and Azure Information Protection](#trace-and-azure-information-protection)
- [Considerations](#considerations)
	- [Usability Considerations](#usability-considerations)
	- [General Infrastructure and Environment Considerations](#general-infrastructure-and-environment-considerations)
	- [Large Workspaces Infrastructure and Environment Considerations](#large-workspaces-infrastructure-and-environment-considerations)
	- [License and Billing Data](#license-and-billing-data)
- [Glossary](#glossary)
- [Appendix A: Trace Object Architecture](#appendix-a-trace-object-architecture)
- [Appendix B: Trace Document Extraction Fields](#appendix-b-trace-document-extraction-fields)
- [Appendix C: Create Email Fields Map Integration Point Profile](#appendix-c-create-email-fields-map-integration-point-profile)
	
	- [Setup Integration Point Profile](#setup-integration-point-profile)

Release Notes
================================

| Trace Version | Release Date     | Relativity Compatibility |
| ------------- | ---------------- | ------------------------ |
| 12.0.8.2      | 24 February 2020 | > 9.6.202.10             |

More information is available in [Relativity Trace Release Notes](https://relativitydev.github.io/relativity-trace-documentation/release_notes)

Introduction to Relativity Trace
================================

Relativity is primarily used for e-discovery, investigations and regulatory inquiries: typically reactive and transactional workflows. Relativity Trace is an application built on the Relativity platform for proactive compliance and surveillance workflows.

Relativity Trace is an [ADS Deployable Application](https://platform.relativity.com/9.5/Content/Building_Relativity_applications/Building_Relativity_applications.htm) containing [RDOs](https://platform.relativity.com/9.5/Content/Managing_Relativity_dynamic_objects/RDO_9.5/Relativity_objects.htm), Custom [Agents](https://help.relativity.com/9.5/Content/System_Guides/Agents_Guide/Agents.htm),
[Event Handlers](https://help.relativity.com/9.5/Content/Relativity/Event_Handler_Express/Relativity_Event_Handler_Express.htm) and other relevant infrastructure.

Prerequisites
=============

Ensure the default Relativity infrastructure has been set up and is fully operational. In addition, Relativity Trace utilizes the following components for its processes:

### Agents

-   dtSearch Index Manager

-   dtSearch Index Worker

-   dtSearch Search

-   Application Installation Manager

-   Auto Batch Manager

-   Integration Points Agent (need to install [Relativity Integration Points](https://platform.relativity.com/9.6/Content/Relativity_Integration_Points/Get_started_with_integration_points.htm?) first)
    
-   Integration Points Manager (need to install [Relativity Integration Points](https://platform.relativity.com/9.6/Content/Relativity_Integration_Points/Get_started_with_integration_points.htm?) first)

If you plan to use `Analytics` functionality, please also make sure the following
agents are set up:

-   Relativity Analytics Index Manager

-   Relativity Analytics Cluster Manager

-   Analytics Categorization Manager

-   Analytics Index Progress Manager

-   Active Learning Manager

-   Active Learning Worker

-   Structured Analytics Manager (need to install [Relativity Analytics](https://help.relativity.com/9.6/Content/Relativity/Analytics/Structured_analytics_set_tab.htm) first)
    
-   Structured Analytics Worker (need to install [Relativity Analytics](https://help.relativity.com/9.6/Content/Relativity/Analytics/Structured_analytics_set_tab.htm) first)

### Applications

-   [Relativity Integration Points](https://platform.relativity.com/9.6/Content/Relativity_Integration_Points/Get_started_with_integration_points.htm?)

    1.  Relativity Integration Points is a required application for Relativity Trace and should be installed in all Trace workspaces BEFORE installing Trace
2.  Used by Trace Data Sources
    3.  See this [page](https://help.relativity.com/9.6/Content/Relativity_Integration_Points/RIP_9.6/Installing_Integration_Points.htm) for details on how to install Integration Points
    
-   [Relativity Analytics](https://help.relativity.com/9.6/Content/Relativity/Analytics/Structured_analytics_set_tab.htm)
    1.  Used by Trace after ingestion to perform Structured Analytics workflows
    (language identification, repeated content identification, etc)
    
### BIST ( Built-in self-test ) Workspace

1.  Create new Relativity Workspace dedicated to BIST. Workspace must be named `Trace BIST Automation [DO NOT MODIFY]`

2.  Follow steps in [Relativity Trace Automated Tests (BIST)](https://relativitydev.github.io/relativity-trace-documentation/bist_smoke_tests)

Setting up Relativity Trace
===========================

1.  Install [Relativity Integration Points](https://platform.relativity.com/9.6/Content/Relativity_Integration_Points/Get_started_with_integration_points.htm?) in all the workspaces that will Run Trace.
    
2. [Install](https://help.relativity.com/9.6/Content/Relativity/Applications/Installing_applications.htm) the `Trace_<version>.rap` from the Application Library tab in the Admin case to all workspaces
   that will run Trace 

   > **NOTE**: Using the Relativity Applications tab from within a workspace to install Trace is NOT recommended. Always install Trace from the Application Library.

3. Wait until application Status switches to `Installed` in the target workspaces


![](media/cada62f5fd9156449b21a32c2a9e34f2.png)
    
3. Create Trace agents

   > **NOTE:** Trace agents are Resource Pool aware.  A single resource pool supports only one `Trace Manager Agent` and an unlimited number of `Trace Worker Agents`

   1.  Trace Manager Agent
       1.  Agent Type = `Trace Manager Agent`
       2.  Number of Agents = `1` 
       3.  Agent Server = Select the agent server you would like the agent deployed
           on (see “Infrastructure and Environment Considerations” section for
           optimal performance)
       4.  Run Interval = `60`
       5.  Logging level of event details = `Log all messages`
   2.  Trace Worker Agent
       1. Agent Type = `Trace Worker Agent`
       2. Number of Agents = `Unlimited`
       3. Agent Server = Select the agent server you would like the agent deployed
          on (see “Infrastructure and Environment Considerations” section for
          optimal performance)
       4. Run Interval = `60`
       5. Logging level of event details = `Log all messages`
   3.  Integration Points Manager Agent
       1. Agent Type = `Integration Points Manager`
       2. Number of Agents = `1`
       3. Agent Server = Select the agent server you would like the agent deployed
          on (see “Infrastructure and Environment Considerations” section for
          optimal performance)
       4. Run Interval = `60`
       5. Logging level of event details = `Log critical errors only`
   4.  Integration Points Agent
       1. Agent Type = `Integration Points Agent`
       2. Number of Agents = `up to 4` (start with 1 and add more if data batches get backed up)
       3. Agent Server = Select the agent server you would like the agent deployed
          on (see “Infrastructure and Environment Considerations” section for
          optimal performance)
       4. Run Interval = `60`
       5. Logging level of event details = `Log critical errors only`

4. Please review the [Considerations](#infrastructure-and-environment-considerations) for system impact information. By default system processes (Tasks) are scheduled to run every 5 minutes (configurable per workspace).

   > Please reach out to `support@relativity.com` for additional information

5. On the `Agents` tab, view the Message of `Trace Manager Agent` until there are no longer any workspaces listed as `Updating` (this is necessary because the manager agent makes additional modifications to target workspaces after application install that are needed in the next steps) ![1571073733941](./media/user_documentation/1571073733941.png)
> **NOTE:** On upgrades, the workspaces with existing data could take considerable time but should not take longer than 20-30 minutes to finish upgrading.  Please reach out support@relativity.com if the upgrade takes longer. 

6. Configure Trace License

   1.  At first install time, a default trial license is installed that is
       re-used for all workspaces in the Relativity instance (it’s valid 30
       days from installation date)

   2.  You can request a new license via `Manage Trace License` link on Setup
       tab

       ![](media/6d24d75c1ed9d35efdc8f0d8e1f9f777.png)

   3.  Click `Request Trace License` and send an email with the contents of the
       request to `support@relativity.com`

       ![](media/741facee0911140b7082894fe5a42c7a.png)

       > **WARNING:** Once license expires, all Trace processes stop working in **all** configured workspaces.

7. In the workspace, navigate to the `Trace`->`Setup` tab and set the `Run Option` to `Continuous`
   
> **WARNING:** Changing the “Run Option” to “Continuous” will automatically build
   a dtSearch index for this workspace for all documents present. Only change this
   setting to "Continuous" when appropriate agent infrastructure is configured and
   disk space available to build a corresponding dtSearch Index. Please reach out
   to `support@relativity.com` for support on installing Trace into workspaces with
   existing data.

Trace Document Flow Overview
============================

Trace has a three-step process that all new documents go through. This status is tracked in the `Trace Document Status` field.

|             | **Step 1**                                                   | **Step 2**                                    | **Step 3**                                                   |
| ----------- | ------------------------------------------------------------ | --------------------------------------------- | ------------------------------------------------------------ |
| **Status:** | **NEW**                                                      | **INDEXED**                                   | **TERM SEARCHED**                                            |
| Overview:   | Documents that are brand new and are part of `Trace All  Documents` saved search | Documents that have been successfully indexed | Documents that have been successfully searched by Term Searching task |

**NOTE:** Rule evaluation (including tagging documents to the Rule) is executed by the Rule Evaluation task and is outside of the core Trace Document Flow.

### Trace Document Fields

As documents flow into a Relativity workspace and through the Trace workflow the
status of documents is reflected on a few key fields on the Document object

1.  **Trace Checkout** – Fixed-length Text field responsible for checking out
    each document for the Trace Data Flow
2.  **Trace Document Status** – Single choice field responsible for reflecting
    overall progress of the document through the Trace Data Flow
    - Standard choices are `1 – New`, `2 – Indexed`, and `3 – Term Searched` reflecting the statuses above
    
    - `Indexing Errored` status reflects documents that have not successfully gone through Indexing. 
    
      - Potential causes
        1. Broken infrastructure (Agents, Service Host issues)
        2. Documents could not be update with `New` status due to SQL outage
    
      - Actions
        1. Check Setup tab for statuses of agents
        2. Check Trace logs (via `Manage Logs` console button)
        3. Perform `Trace Document Retry` mass-operation on affected documents
    
    - `Searching Errored` status reflects documents that have not successfully gone
      through Term Searching stage.
    
      - Potential causes
        1. Broken infrastructure (Agents, Service Host issues)
        2. Permanently broken/invalid Term (dtSearch) syntaxes are present
    
      - Actions:
        1. Check Setup tab for statuses of agents
        2. Check Terms tab for detailed errors on each failed term
    
        3. Check Trace logs (via `Manage Logs` console button)
    
        4. Perform `Trace Document Retry` mass-operation on affected documents
5.  **Trace Document Terms** – Multi-Object field tracking which Terms have
    matched a document
6.  **Trace Document Rule Terms** – Multi-Object field tracking Rule specific
    Terms (terms that are associated with any rule) that have matched a document
7.  **Trace Has Errors** – Boolean (yes/no) field indicating if the document has
    any errors related to ingestion, extraction
8.  **Trace Error Details** – Long Text field capturing the error details if a
    document has encountered any errors
9.  **Trace Data Transformations** – Multi-Object field tracking what data
    transformations have been applied to the document as part of ingestion
10.  **Trace Monitored Individuals –** Multi-Object field tracking which Monitored Individuals are associated with each document 
11.  **Trace Rules –** Multi-Object field tracking which Rules of type Alert matched a document
10.  **Trace Workflow Rules –** Multi-Object field tracking which Rules of type Workflow matched a document
11.  **Trace Matching Rules (INTERNAL)** - Multi-Object field used by the Rules engine to snapshot documents that match the Saved Search and Term conditions for each rule execution. Please do not edit or use this field for any other purpose as the data within it is cleared and updated frequently.
12.  **Trace Record Origin Identifier -** Contains an identifier (varies by Data Source) that can be used to reconcile Trace documents with their origin
13.  **Trace Data Batch -** Tracking object that shows when and how the document was brought into the Workspace
14.  **Trace Data Batch::Data Source -** System generated field that can be used to show the Data Source that created each document. If desired, edit this field to Allow Pivot and Allow Group By and place a Widget on the Documents dashboard to see how many documents are generated by each Data Source.

### Dashboard Widgets

You can get a quick understanding of the status of your system and documents by
applying Widgets using Trace Document Fields to the Documents dashboard:

![](media/233f58be6430edea9858817e9d1aa6d9.png)

Trace Document Retry and Error Resolution Workflow
-----------------------------------

If you wish to re-submit existing documents through the Trace Data Flow, you can
accomplish this via Document mass operation `Trace Document Retry`. The mass
operation resets the following fields: `Trace Checkout`, `Trace Terms`, `Trace Rule Terms`,
`Trace Rules`, `Trace Workflow Rules` and `Trace Document Status`. Simply check the
documents that you wish to retry from the Document List and click the item in the
dropdown and click `Ok` on the pop-up. If your browser settings prevent pop-ups please
enable them for Relativity URLs.

> **Note:** Trace Document Retry does not re-extract metadata from the document natives. Existing data is used to re-index, and re-run term searching and rule evaluation using the current term and rule sets.

> **Note:**" Trace Document Retry will only work when the "Run Option" on the Setup tab is set to `Continuous`

>  **WARNING:** The retry process can be very resource-intensive. Trace is optimized for ongoing and forward-looking use cases where documents are only searched once upon ingestion. Triggering a retry will treat affected documents as if they were brand new to Trace, clearing all previous Rule and Term
> associations. If enough documents are retried at once, the system could struggle to handle the sudden influx of documents. Please exercise caution when using this feature.

![](media/b67f9b74d4a53cdd71c6d2915c81830d.png)

![](media/915f9beb5aa8a35c4a90aae5f23d548c.png)

Trace Rules Engine Overview
===========================

The Trace Rules Engine allows users to define data buckets with specific
triggers and actions. These Rules are executed on periodic basis at
pre-configured intervals, allowing users to automatically categorize and tag
documents as they are ingested into the workspace.

1 - Creating a Rule
-------------------

Create a new [rule](#_Glossary) by clicking `New Rule` on the `Trace`:`Rules` tab

![1571081144550](media/user_documentation/1571081144550.png)

The Rule Creation form contains the following fields:

-   **Rule Name:** the name of the rule

- **Rule Type:** classification of the purpose of the rule

  -   **Alert:** a rule that is designed to alert on responsive documents. Documents matching rules of this type are tagged in the Trace Rules field.
  -   **Workflow:** a rule that is designed to operate on every document in the workspace and take some action (run a script, move to the correct folder, etc) but not to alert on responsive documents. Documents matching rules of this type are tagged in the Trace Workflow Rules field.

-   **Searchable Set:** the document set searched when the Rule runs. Make sure
    this set contains all documents that should be considered by Trace Rules.

-   **Associated Actions:** actions are what happen when a rule matches a
    document. Currently supported Action Types are:

    -   **Data Disposal**: triggers deletion of specified documents after the
        configured retention policy
    -   **Advanced:** execute customer provided Relativity Script
    -   **Email:** generates an email with metadata about alerted documents
    -   **Webhook:** makes a generic API call hosted within Relativity
    -   **Slack:** generates a Slack message with metadata about alerted
            documents
    -   **Move To Folder:** Move matched documents to a specific folder
> **NOTE:** The Tagging action type has been deprecated. Documents are always tagged automatically with the associated Rule (happens as the final action as part of Rule Evaluation, if a document is tagged then all of the actions on the rule were executed) on either the Trace Rules or Trace Workflow Rules field, depending on the Rule Type

> **NOTE:** The Batch action has been deprecated. You can still create a Batch Set manually from any saved search and set it to `auto-run`.

2 - Customizing and Running a Rule
-----------------------------------

After you create a Rule, use the Rule definition page to associate Terms and Enable/Disable the rule.

![1571079859530](media/user_documentation/1571079859530.png)

##### Terms

Associating Terms with a Rule causes it to only target documents that contain one or more of the associated terms in their extracted text. To associate Terms with a Rule, use the New or Link buttons on the Rule Layout. Please see the Terms section below for more information on creating Terms.

##### Enable / Disable

Clicking the Enable Rule console button on the right of the layout causes the Trace Manager Agent to run the rule on an ongoing basis. Clicking Disable Rule will stop future execution of the Rule.

> **NOTE:** A Rule will only execute when it is enabled. If a Rule is disabled while running, it will still finish its current execution.



3 - Validating Results
----------------------

After Rule executes, documents matching the Rule will be associated to the Rule
itself.

![1571081067899](media/user_documentation/1571081067899.png)

Terms
-----

In addition to metadata conditions of a Saved Search associated with Rule, you can apply extensive searching criteria to isolate only the most relevant documents.

### Creating Terms

You can create Terms in multiple ways:

1.  Via [Remote Desktop Client ( RDC )](https://help.relativity.com/9.6/Content/Relativity/Relativity_Desktop_Client/Relativity_Desktop_Client.htm) load file. This method is ideal if you are adding a lot of terms at once.
    
2.  From Terms tab by clicking “New Term” and adding each Term individually.

3.  By clicking the New button on the Terms section of the Rule Layout.

![](media/e2a523416ac9607f8b2e9f42e2287e0f.png)

Term definition contains 3 fields:

-   **Term:** searching string (unique identifier)
-   **Term Category:** optional group name to organize terms
-   **Score:** optional field to populate term weight to indicate importance
-   **Relativity highlight color:** optional highlighting configuration in the
    Relativity viewer (see [highlighting](#highlighting) section)

In addition, you can see and modify Term Categories and Rules associated with
Term and its status with regards to execution

![](media/5b46e7806548749e50586196d43aa468.png)

> **NOTE:** The Term “Name” (actual text being searched) **cannot** be modified after it is created. You must remove and add a new term object to change the search string. You **can** modify the highlight color and term category of an existing term.

> **NOTE: The** Term “Name” (actual text being searched) is limited to 450 characters. Please reach out `support@relativity.com` if your use-case requires higher limits for your terms.

### Highlighting

By default, Trace creates a `Trace Persistent Highlight Set` that is populated
with **all** terms present in the workspace. In addition, the `Trace Rules
Persistent Highlight Set` captures only terms currently associated with any
Rule. You can have many terms you want to use for highlighting purposes only,
and not necessarily as part of matching any specific Rule. You can adjust the
order in which they are displayed in the viewer.

![](media/bde02ea5c3426235025046047f062ca8.png)



![](media/9d27aea9a85e6309995c771fc88f6ca7.png)



In addition, you can override default highlight configuration (magenta background and black text) by specifying a semi-colon separated list of pre-configured color combinations. The details of the color codes can be accessed via Context Help button on the Term Definition page.

![](media/587ff246e9bf742376893bde0d0469ab.png)

Actions
-------

As part of installation, Action Types are created. Currently supported action types are: Move To Folder, Data Disposal, Advanced, Email, Slack, and Webhook. For each Action Type there is a Default Action created. You can customize the Default Actions, but it is recommended that you create and configure your own Actions.

### Move To Folder Action Type

Move To Folder action works on documents that match Rule criteria. Upon execution of the action, the documents will be moved to the specified destination folder, inheriting folder permissions. This action can be used to effectively secure documents to specific set of users/groups by routing them to folders with
different permission sets. This Action Type can also be used to drive regional review workflows in conjunction with alerting actions. Finally, it can be used to filter documents by moving only the relevant documents to a different folder targeted by additional rules.

You can configure the action by specifying the Artifact ID of the destination folder where the documents are to be moved. The Additional Information section automatically populates the list of available folders and their corresponding Artifact IDs for your convenience.

![](media/4ea5493c6871466e5ab385ec5c08f8ff.png)

### Data Disposal Action Type

> **WARNING:** The Data Disposal Action will permanently delete all documents that match the Rule conditions if they are outside the Data Retention window.
>
> Data Disposal will only delete files located in valid Data Batch folders. Documents not ingested by Trace will not be deleted.
>
> If a Document is disposed, but not its parent or children, only the disposed Documents files are deleted.
>
> _Only_ documents which were imported as part of a Data Batch which is in the `Completed` state will be deleted.

The Data Disposal Action Type follows the same Trace Rules Engine paradigm with
one added condition:

-   You attach the Data Disposal action to a Rule

-   The rule executes the actions on documents that match the Rule conditions

    -   Searchable Set

    -   Term Searching (optional)

    -   *SPECIFIC TO DISPOSAL*: `Delete Documents Older Than Hours` setting

Any document that is newer than the specified number of hours (based on the System
Created On field) will not be deleted even if the document is included in the
Searchable Set / Search Term.

This action is used to:
1. Clear out old documents from the Trace workspace
2. Free disk space by deleting **ALL** disposed Document files from the File Share

**Action Configuration**

`Document Delete Batch Size` - controls number of documents to delete at once
> **Note:** All documents will be deleted in a single pass, this is a tweak to improve SQL performance

`Files On Disk Document Delete Page Size` - controls the number of Relativity Documents to process file deletion for at a time

`Delete Documents Older Than Hours` - controls age of a document (based on System
Created On date/time) to delete

**Saved Search Recommendations for Data Disposal**

Because of the risk of data loss. You should carefully configure the Searchable Set used for Data Disposal. The following are recommended minimum filtering parameters

-   `Trace Has Errors` field is False
-   The `Alert` field is not set (is empty)
-   A field marking the document as Reviewed is True

### Advanced Action Type

> **WARNING:** Advanced Action type can execute potentially harmful Relativity
Scripts. Apply rigorous testing and impact assessment prior to deploying any
custom script in production or enabling it to run continuously via Trace
automation. For more information about Relativity Scripts in general, see the
[Scripts](https://help.relativity.com/9.6/Content/Relativity/Scripts.htm) and [Scripts Properties](https://platform.relativity.com/9.6/Content/Scripts/Script_properties/Script_properties.htm) documentation pages.

The Advanced Action Type executes a Relativity Script automatically on a
recurring basis per the Rule Evaluation task configuration (schedule).

To create a Rule with an Advanced action attached, follow the following steps:

**Step 1**: Identify a Relativity Script you want to run automatically or create
one yourself.

-   Script must have Saved Search input parameter with *name* attribute “Saved
    Search”

-   For more information about Relativity Script feature in general, see the
    [Scripts](https://help.relativity.com/9.6/Content/Relativity/Scripts.htm) and [Scripts Properties](https://platform.relativity.com/9.6/Content/Scripts/Script_properties/Script_properties.htm) documentation pages.

> **NOTE:** Currently all Relativity Scripts that can be associated with Trace
actions require a Saved Search as one of the script inputs (Trace automatically
populates that field during Rule Evaluation with an execution specific saved search).

![](media/b993eb14ebd4e458fc490b9bcb321405.png)

**Step 2**: To incorporate your script into the Rule framework, you must create
an Action of Action Type “Advanced”.

-   You can create new advanced actions in the Trace:Actions tab.

-   This custom Action will eventually be attached to a Rule, so be sure to give
    it an easily identifiable name.

![](media/6ff2d4797b8a580e607a6f1b5ad4c6c1.png)

Configuration of the action needs to provide all needed script inputs in the
following format:

```json
{
"Destination Hour Of Day Field Name": "TraceHourOfDay",
"Destination Day Of Week Field Name": "TraceDayOfWeek",
"Timezone": "Central Standard Time"
}
```

For reference:

-   “Trace Date Parser” is the displayed script name as seen in the UI

-   “Destination Hour of Day Field Name”, and “Destination Day of Week Field
    Name” are Scripts Input Names

-   “Trace Hour Of Day”, and “Trace Day Of Week” are SQL Column names of
    corresponding Relativity Fields

> **NOTE:** Trace will automatically create a Saved Search that returns the net-new documents
from your chosen Saved Search (from the Rule) AND your Term conditions and use that as 
the input for the script.

**Step 3**: To execute your script, attach this Action to a Rule and enable it
(as you would with any Rule).

> **NOTE:** Advanced actions will run on a schedule, continuously. Please consider the resource
usage of your scripts!

### Alert Action Types (Email, Slack, and Webhook)

Trace supports the following modes of notification: Email, Slack, and Webhook.
These actions can be used as part of any rule.

#### Replacement Tokens

You can specify Trace Replacement Tokens in most configuration fields for the
Alert Action Types. These tokens will be replaced with information relevant to
the specific document, rule and/or alert:

`<<TRACE_RULE_VIEW_LINK_TOKEN>>` - Link to view Trace Rule in Relativity that generated the alert

`<<TRACE_DOCUMENT_VIEW_LINK_TOKEN>>` - Link to view Document matched by the alert

`<<TRACE_DOCUMENT_IDENTIFIER_TOKEN>>` - Relativity document identifier (Control Number)

`<<TRACE_DOCUMENT_ARTIFACT_ID_TOKEN>>` - ArtifactID of the document matched by the alert

`<<TRACE_RULE_NAME_TOKEN>>` - Name of the Trace Rule that generated the alert

`<<TRACE_WORKSPACE_ID_TOKEN>>` - Relativity Workspace ID (ArtifactID) of the workspace that generated the alert

> **NOTE:** The Trace Relativity Replacement tokens are not configurable, and are the only tokens available in the Trace application.

#### Email Action Type

You can configure the Email action to send out an email about specific document matching rule conditions. Note that the Email Action uses the SMTPUserName, SMTPPassword, SMTPServer, SMTPPort and SMTPSSLisRequired settings from the kCura.Notification Section of Instance Settings to send your email messages, so be sure they are configured properly.

![](media/user_documentation_EmailAction.png)

**Configuration**

`Email Settings – Body Template`: Customize text that appears above the list of alerted documents. Can insert custom HTML and Replacement Tokens.

`Email Settings – Subject Template`: Subject of the email

`Email Settings – Recipients`: Recipients of the email. Supports to/cc/bcc, example (include square brackets): 

```[to:email1@test.com,cc:email2@test.com,bcc:email3@test.com]```

`Email Settings – From Address`: Sender of the email

`Document Link` - By default contains a link to the alerted document. Can insert custom text and Replacement Tokens.

`Document Text` – Text that appears next to the Document Link. Can insert custom text and Replacement Tokens.


##### Sample Email generated by Trace

![](media/1de78345e5e05d118e47e14bdabbe7e5.png)


#### Slack Action Type

![](media/0ac2a1861d1669e683f07a75768fc13e.png)

**Configuration**

`Slack Settings - Channel` – Slack channel to use for alert

`Slack Settings - Message Template` – Customize text that appears above the list of alerted documents. Can insert Replacement Tokens.

`Slack Settings - User Name` – Sender of the Slack message

`Slack Settings - Slack Base Url` – URL to use for Slack alert

`Slack Settings - Slack Web Hook Id` – Unique ID of the registered incoming webhook

`Document Link` - By default contains a link to the alerted document. Can insert custom text and Replacement Tokens.

`Document Text` – Text that appears next to the Document Link. Can insert custom text and Replacement Tokens.


> **NOTE:** You must register a webhook to specific channel in Slack that will be
allowed to post messages from Trace. Once registered, enter the ID of the
registration into “Slack Web Hook Id” field. For more information visit: <https://get.slack.help/hc/en-us/articles/115005265063-Incoming-WebHooks-for-Slack>

##### Sample Slack message generated by Trace

![](media/cd2e4f2e817bf98137c500a0dffb5bab.png)

#### Webhook Action Type (Preview)

You can configure an action to make an API call to any web services hosted
within the Relativity infrastructure. For example, you could use the Webhook
action to call RelativityOne notification API to generate a mobile alert for the
documents.

![](media/4efb7e0f6f2fc5514e4b29efbf2c32d1.png)

**Configuration**

`Json Payload` – Custom JSON string containing the payload of the API call. Can insert Replacement Tokens.

`Web Hook Base Api Url` – URL of the Relativity instance hosting the API

`Web Hook Api Function` – relative path of API method to execute (used in
combination with Base Api Url)

`Document Link` - By default contains a link to the alerted document. Can insert custom text and Replacement Tokens.

`Document Text` – Text that appears next to the Document Link. Can insert custom text and Replacement Tokens.


> **NOTE:** POST is the only supported HTTP verb for Webhook.

> **NOTE:** Authentication is inherited from the Agent Server that is hosting the
Trace agent. The access_token is retrieved from
ClaimsPrincipal.Current.Identities.

### Custom Relativity Scripts

Several useful SQL Relativity Scripts are shipped by default with Trace
application.

| **Script Name**   | **Description**                                              | **Inputs and Outputs**                                       |
| ----------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Trace Date Parser | This script parses system CreatedOn Date Time field into a Trace Day Of Week field and Trace Hour Of Day field | **INPUT:** Timezone<br>**INPUT:** Saved Search to execute on (passed from Rule)<br>**OUTPUT:** TraceHourOfDay Field Name<br>**OUTPUT:** TraceDayOfWeek Field Name <br><br>The SQL Query ```SELECT * FROM sys.time_zone_info``` will return all time zones available on the SQL Server. Use any of the Time zone names in the `Timezone` Input. |

Trace Proactive Ingestion Framework
===================================

The Proactive Ingestion Framework allows Administrators to automatically and
continually ingest data into Relativity from various Data Sources. The framework
is built on top of [Relativity Integration Points](https://help.relativity.com/9.6/Content/Relativity_Integration_Points/RIP_9.6/Installing_Integration_Points.htm).

The key benefits of the Proactive Ingestion Framework include:

-   Data reconciliation from Data Source through transcription/normalization to
    Relativity

-   Data can be autonomously and continuously ingested

-   Data can be pushed from a 3rd party data processor in a generic format

-   Data is broken into batches improving stability and throughput

-   Data can be manually previewed to facilitate field mapping troubleshooting

-   Data can be re-imported into Relativity at any point (asynchronously from
    retrieval from the Data Source)

-   Data can be transformed by replacing redundant / irrelevant blocks of text
    or removing duplicate documents from consideration

-   Performance monitoring of entire data ingestion pipeline (bottleneck
    identification, SLA metrics, proactive alerting)

Reach out to <support@relativity.com> for help integrating with the Proactive
Ingestion Framework

Data Sources
------------

Data Source is a Relativity Dynamic Object (RDO) that ships with Trace application. It allows you to define where/how you are pulling data. The Data Source references the Integration Point Profile that holds configuration on how to import data for that Data Source (field mappings). Data Batches reference
Data Source to dynamically lookup which Integration Profile to use during import.

![](media/21c4697ae7410c35b716e82c5a65937f.png)

The following are fields on the Data Source Object:

-   **Name:** The name of Data Source

-   **Data Source Type:** Type of the data source

-   **Integration Point Profile:** Integration Point Profile used to load data
    from this Data Source

-   **Username:** Optional field used for authentication to a data source

-   **Password:** Optional field used for authentication to a data source

-   **Start Date:** Date from which data will be pulled/pushed into Relativity

-   **Last Runtime (UTC):** The timestamp when this Data Source was last
    executed

-   **Status:** The last status message recorded by the Data Source

-   **Last Error Date:** Timestamp of the last time this Data Source failed, if
    it happened recently (based on Last Error Retention in Hours setting under
    Data Source Specific Fields)

-   **Last Error:** Error message from the last time this Data Source failed, if
    it happened recently (based on Last Error Retention in Hours setting under
    Data Source Specific Fields)

-   **Data Source Specific Fields:** this section defines input fields specific
    to particular data source

-   **Trace Monitored Individuals:** Configures which monitored individual’s
    data should be retrieved from the data source

-   **Data Transformations:** Determines which Data Transformations (replacement
    or deduplication) will be applied to the data retrieved by this data source

-   **Data Batches:** Data Batches generated by this Data Source

-   **Enable/Disable Data Source (Console Button):** Enables (or disables) data
    retrieval for particular data source

-   **Reset Data Source (Console Button):** Disables and resets data source to retrieve data from specified Start Date (Depending on import profile settings, enabling this data source after a reset could duplicate data in the Workspace)

### Data Source Specific Settings

This section contains additional settings which are not associated with specific
Relativity Fields. The settings described here are common across all Data Source
Types. Type-specific settings are documented under their respected Data Source 
sections.

-   **Password Bank** Used to specify known passwords to attempt
    while encountering protected native files. Multiple passwords 
    can be separated by the pipe character, |. Passwords containing
    the pipe character are supported through escaping the pipe character
    with a second pipe. Pipes are always escaped left to right.

    > **Example Password Bank:** passw0rd|Trace1234!|aaa|bb|cccc||dd||eee|||ff|||ggg||||hhh|||||
    >
    > Yields the following passwords:
    >
    > * passw0rd
    > * Tracer1234!
    > * aaa
    > * bb
    > * cccc|dd|eee|
    > * ff|
    > * ggg||hhh||

- **Extraction Thread Count:** The number of documents to extract in parallel.

- **Enrich Documents:** Whether or not to extract metadata and children from original documents. Valid values: true/false

- **Discover Monitored Individuals:** See [Discovery of Monitored Individuals](#discovery-of-monitored-individuals)

- **Discover Monitored Individuals Ignores Case:** See [Discovery of Monitored Individuals](#discovery-of-monitored-individuals)

- **Last Error Retention In Hours:** The length of time to persist any message in the `Last Error` field.

- **Aip Application Id:** See [Trace and Azure Information Protection](#trace-and-azure-information-protection) 

- **Aip Tenant Id:** See [Trace and Azure Information Protection](#trace-and-azure-information-protection)

### Data Source Auto-Disable

Trace will automatically disable data sources that are identified as unhealthy or have critical configuration errors that will require intervention by the user. Trace will automatically disable a data source for the following reasons:

- Data source has not had any successful data batches in a configured amount of time (default 24 hours)
- Globanet data source is enabled without enabling Globanet at the workspace level

Auto-disabled data sources will have their Disabled Reason field populated to show that it was disabled by the system. The data source will also have error details outlining the failures that caused the system to disable it. 

### Microsoft Exchange Data Source

The Microsoft Exchange Data Source enables Relativity to automatically pull emails from a Microsoft Exchange instance (Office 365 or On Premises) into Relativity. The Microsoft Exchange Data Source is executed by the Data Retrieval task (seen on the Setup tab). Note, this Data Source only pulls emails at this
time, if you need to retrieve other object types from Microsoft Exchange please reach out to `support@relativity.com`

**Data Flow Overview**

![image-20200120151709677](media/user_documentation/image-20200120151709677.png)

**Setup**

**Step 1: Create Integration Point Profile**

>   Refer to **[Appendix C](#appendix-c-create-email-fields-map-integration-point-profile)**

**Step 2: Adjust Office 365 permissions**

>   **NOTE:** Settings for On Premises exchange are very similar to Office 365

1.  Log into the Office 365 Admin Center

2.  Adjust Administration Exchange settings:

    ![](media/0fd5f7977b9d30dbb67296d148efc4c5.png)

3.  Under Admin Roles create (or update if exists) Discovery Management role:

    ![](media/857028bf0af361bf0d37d007d8e08672.png)

4.  Ensure the account you use to authenticate with includes “Application
    Impersonation”, “Legal Hold”, “Mailbox Import Export” and “Mailbox Search”
    roles:
   
   ![](media/aff48bb585413bf91fa03d1649933aab.png)

5. (Optional) Adjust password expiration permission for the account used for Trace

     https://docs.microsoft.com/en-us/office365/admin/add-users/set-password-to-never-expire?view=o365-worldwide#set-the-password-expiration-policy-for-individual-users

**Step 3: Create a Microsoft Exchange Data Source**

1.  Go to the Trace:Data Sources Tab and Click the “New Data Source” Button

2.  Set the Name = “*Microsoft Exchange*” (for example)

3.  Select Data Source Type: “Microsoft Exchange”

    ![](media/45f8a402b934539e3a94ed74d11081b3.png)

4.  Select Integration Point Profile created in **Step 1**

5.  Set the Username = username (usually an email address) of the admin user you
    will be using to retrieve emails (configured in Step 2)

6.  Set the Password = password of the admin user you will be using to retrieve
    emails

7.  Set Start Date to the earliest email timestamp you would like imported (UTC
    time)

8.  Under Data Source Specific Fields, set Exchange Settings - Url and Exchange
    Settings - Version (there are a lot of other settings that can be
    configured, but the default values are fine, please contact us if you would
    like more information)
<img src="media/user_documentation/image-20200226224716411.png" alt="image-20200226224716411" style="zoom:50%;" />
    
1.  *Exchange Settings – Url* gives you the chance to specify the exact URL
        used when connecting to your exchange server. If this field is left
        blank, Microsoft’s Autodiscover technology will be used to populate the
        field with a URL based on the credentials provided in the Username and
        Password fields. Autodiscover is typically a suitable option and works
        for Office 365 and many on premises solutions but it is not guaranteed
        to work.
        
	
	    If Autodiscover fails, specify this URL in the field: https://outlook.office365.com/EWS/Exchange.asmx ( OR https://YOUR_EXCHANGE_SERVER_URL/EWS/Exchange.asmx)
	
2. *Exchange Settings - Version* allows you to specify the version of your
       exchange server. For Office 365, the default is the correct choice. For
       on premises servers, provide the correct version. It needs to be an
       exact match to one of the options, filling it out incorrectly will
       provide a list of all of the options available in the error message at
       the top of the page: Exchange2007_SP1, Exchange2010, Exchange2010_SP1,
       Exchange2010_SP2, Exchange2013, Exchange2013_SP1
   
9. Click “Save”

10. Link / Create New Monitored Individuals (same page after clicking Save)

    ![](media/85e99ebffc8ada7ae4c69a61cb873213.png)

    1.  Click `New` if the monitored individual is not already defined on another
    Data Source, or “Link” if the user has already been monitored in the past

    2.  Microsoft Exchange Data Source will only pull data for linked Monitored
    Individuals (by identifier field: email address)

    3.  Once everything is set up, click the Enable Data Source button on the upper
    right to begin pulling data

**Content**

The Microsoft Exchange Data Source works by pulling content directly from an
Exchange Server instance (Office 365 or On Premises) using Exchange Web Services
(EWS). The Data Source downloads the native (.eml) email files and then extracts
all information including email metadata, email body text, native attachments
and their metadata. Container attachment file types (zips and similar archives)
are automatically extracted into individual documents – e.g. zip with 10 word
(.docx) documents = 11 Relativity documents. In addition, images from email
content and each individual document are automatically expanded into separate
Relativity documents. 

> **NOTE:** the Microsoft Exchange data source only retrieves emails. It does not retrieve other exchange metadata at this time.

Please, refer to [Appendix B: Trace Document Extraction Fields](#appendix-b-trace-document-extraction-fields) for field descriptions.

### Relativity Native Data Extraction Data Source

This Data Source allows for automatic text extraction/expansion of previously
ingested documents with natives in Relativity. This data source will
automatically extract text, metadata and any children documents from
containers/archives for all documents in the workspace with *Trace Data
Enrichment Needed* field set to *Yes* and where Trace is able to locate the Native
file on disk:

![](media/055bc4b791f13c0bdcb07bed1d907b91.png)

**Setup:**

1. Integration Points Profile

   1.  Please, re-use profile creation steps documented for Microsoft Exchange
       above OR re-use existing “Microsoft Office 365 Profile” profile. 
       
       > **IMPORTANT:** Ensure import option is set to Append/Overlay.

2.  Create Relativity Native Data Extraction Data Source

    1.  Go to the `Trace` -> `Data Sources` tab and Click the “New Data Source” button

    2.  Set the Name = for example, “Native Data Extraction”

    3.  Select Integration Point Profile created in Step 1

    4.  Select Data Source Type: “Relativity Native Data Extraction”

    5.  Ignore Username field

    6.  Ignore Password field

    7.  Ignore Start Date field

    8.  You have the option to leave the Data Source as Enabled or Disabled

3.  Fill out [Data Source Specific Settings](#data-source-specific-settings) and click Save
    
    -   **Batch Size:** The maximum number of Original Native files 
        to group into a single Data Batch

**Content**

Extracted text and metadata for submitted Native files and all children
documents expanded from containers/archives. Please, refer to [Appendix B](#appendix-b-trace-document-extraction-fields)
for field descriptions.


> **WARNING:** Re-extraction of child documents from containers (emails, zips, archives) will generate duplicate child documents (old children will be dropped off the family group) if they already exist in the workspace.

>  **WARNING:** Containers with many children documents (and nested containers) could produce significant number of expanded items in Relativity.

**Limitations**

Relativity Native Data Extraction Data Source do not support [Deduplication](#deduplication-data-transformation). Deduplication transformations must be unlinked before the Data Source can be enabled.

Monitored Individuals
---------------------

Trace Monitored Individual is a Relativity Dynamic Object (RDO) that ships with
Trace application. It allows administrators to define an individual that can be
monitored by a Data Source by importing data that belongs to them. Monitored
Individuals can be linked to multiple Data Sources. Each individual Data Source
has its own logic to determine what data is retrieved based on the linked
Monitored Individuals. Monitored Individuals are also used as a unit of billing
by Relativity Trace. Generally a Relativity Trace license will specify a number
of Monitored Individuals available and the number of data sources they can be
used on.

>  **NOTE:** The only two fields on Monitored Individual currently used in application logic are the `Identifier` and `Secondary Identifier` fields. All other fields are simply for display purposes. Each Monitored Individual must have a unique value in the Identifier field. Typically the Identifier is the employee’s email address. Identifier is **case-sensitive** (e.g. `Test@test.com` and `test@test.com` are treated as two different email addresses / identifiers). The Secondary Identifier field is used to list other email addresses that may be associated with this Monitored Individual. Email addresses in the Secondary Identifier field should be delimited with a semi-colon (;).

Data Transformations
--------------------

Trace Data Transformation is a Relativity Dynamic Object (RDO) that ships with Trace application. It allows administrators to specify a way data should be transformed while it is being imported by the data source. Currently there are two types of Trace Data Transformations, Replace and Deduplication. Trace Data
Transformations are attached to a Data Source by clicking Link in the Data Transformations section of the Data Source Layout.

### Replace Data Transformation

Data Transformations of type *Replace* allow you to strip exact blocks of text out of the Extracted Text for any documents imported by the associated Data Source. This allows for removal of things like email signatures and other frequently occurring, benign text so that they do not match Terms. A Data Source
can be associated with multiple Data Transformations of type Replace.

By Default, content will simply be replaced with nothing (empty string). This can be changed by entering Match Replacement Text in the Configuration section.

![](media/5c3b02bf2c121e73f3a3916db799b306.png)

> **NOTE:** Replacement of the Extracted Text (application of replace data transformation) happens on a new copy of the Extracted Text (referenced as *<original_file_name>.replaced.txt*).  Newly generated Extracted Text is referenced in a separate loadfile (`loadfile.replaced.dat`).  In addition, if original loadfile.dat contains a column named `Extracted Text Size in KB` , ONLY then `Extracted Text Size in KB` field is re-generated with updated length.  **Previously generated/supplied data for `Extracted Text Size in KB` is overwritten in this case.**

### Deduplication Data Transformation

Data Transformations of type *Deduplication* prevent a Data Source from importing a document if the same document already exists in the workspace. Only one Data Transformation of type Deduplication should be associated with each Data Source.

For Trace native [Data Sources](#data-sources),  deduplication is driven by a SHA256 hashing algorithm that populates the Trace Document Hash field on each document. By Default, if the document is an email, the algorithm will hash together the sender, subject, recipients, sent date, email body and attachment list to create the hash value. If the document is not an email, then the hash will be done directly on the bytes of the file. It is possible to configure the exact hashing algorithm used for emails using the settings in the Configuration section:

![](media/70b0ae9c6debe35956d4988dffaae892.png)

When additional documents are ingested (either within the same Data Batch or different Data Batches), hashes will be compared to those on documents that already exist in the workspace. If there is a match, the duplicate document will not be ingested. Instead, the Trace Monitored Individuals field on the document
will be updated to include the Monitored Individual that was the source of the duplicate in addition to the Monitored Individual that was the source of the original.

#### Required Fields for Deduplication

Deduplication of a Data Source requires that the following Relativity fields be mapped in the Data Source's associated Integration Point Profile.

1. Trace Document Hash
2. Group Identifier

### Group Identifier Truncation for External Data Sources

`Group Identifier` is a special field in Relativity Trace that is used to power several features including Deduplication. It is essential that a value for `Group Identifier` be provided for every document imported with Trace. Relativity imposes a restriction on the Group Identifier field where the value is not allowed to be longer than 400 characters. The Trace team has found that some external Data Sources populate Group Identifier with a value longer than 400 characters. Instead of failing to import documents from these Data Sources, if the value provided in the field mapped to Group Identifier is longer than 400 characters, Trace will calculate the SHA256 hash of the value and use the hashed value instead. If Group Identifier Truncation occurs, the document is marked as `Trace Has Errors` and the `Trace Error Details` field is filled with a message explaining that a hashed value was used instead of the original Group Identifier value provided.  The message template is of the following format: `{groupIdentifier_SourceFieldDisplayName} length ({groupIdentifierString.Length}) exceeded 400 characters - used hashed string instead`

> **NOTE:** `Group Identifier` Truncation occurs for EXTERNAL DATA SOURCES ONLY. External data sources have a `Provider` on their `Data Source Type` that is not equal to `Trace` or `Globanet`. For additional information, please contact support@relativity.com.

Data Batches
------------

Data batch is a unit of ingestion work for Trace. It corresponds to a load file on disk that needs to be imported with specific settings and field mappings. It is a central tracking object for how the data was generated, normalized and aggregated into a load file. It provides status of the overall import process and
allows for a deep audit history trail from native data source to Relativity.

Once a batch begins the Ingestion process (when status is set to: `ReadyForImport`), the Ingestion task will create an integration point from the Data Source's configured Integration Point Profile (this information includes import settings and field mappings).

> **NOTE:** Trace will only create Integration Points in each workspace up to the number specified in the `Max Simultaneous Import Jobs` setting on the Ingestion Task (default 25). Once that number of Integration Point jobs are created, additional data batches will stay in the `ReadyForImport` status and no additional Integration Point jobs will be created until enough jobs finish to bring the total back under the specified maximum.

![](media/5a4b23b008d4e39bc9bafce213515337.png)

### Data Batch Retry and Error Resolution Workflow

By default, Data Batches that do not complete will be automatically retried up to 3 times. Data Batches that fail all retries will set `Has Errors` to true, populate the `Error Details` field with the details of the specific error encountered, and be given a status of `CompletedWithErrors`.

> **NOTE:** If a Data Batch completes successfully but has errors at the document level (for example, if a document is password protected and the correct password was not found in the Password Bank of the Data Source), the Data Batch will be marked `CompletedWithDocumentLevelErrors` and there will not be an automatic retry

Data Batch objects have associated Mass Operations (and corresponding Data Batch console UI buttons) to help with state resolution

1. `Trace Data Batch Retry` – submit the Data Batch to be retried by Trace. This reverts the Data Batch to the `RetrievedFromSource` status and Trace will once again attempt to ingest the data.

   > **Warning:** `Trace Data Batch Retry` will create duplicates of documents that were imported on previous attempts if deduplication is not enabled on the Data Source.

2. `Trace Data Batch Abandon` – update the Data Batch to indicate that it has been manually resolved and that no further work needs to be done. Using this action is necessary when errors are resolved manually because otherwise the Ingestion task will continue to report the presence of Data Batches in the CompletedWithErrors status.

   ![](media/fafdd5aacec029271e4f39ca303c80fa.png)

> **NOTE: ** If a Data Batch sits in a status other than `Completed`, `CompletedWithErrors`, `CompletedWithDocumentLevelErrors`, or `Abandoned` for longer than 24 hours (timeout configurable with the `Data Batch Timeout In Hours` setting on the Data Validation Task), it will automatically be marked as:
> * `CompletedWithErrors` if the Data Batch has files and could be retried
> * `Abandoned` if the Data Batch does not have any files. 
>
> This functionality helps ensure that temporary system issues do not lead to Data Batches being stuck indefinitely containing documents that never make it into the workspace.

Discovery of Monitored Individuals
--------------------------------------------

Some Data Sources combine data from several places into a single import flow. In that scenario, it may not be clear which Monitored Individual is the source of a given document and no Monitored Individual will be tagged. To address this issue, Trace has introduced the `Discover Monitored Individuals` option on every Data Source. If enabled, Trace will look inside of the document and tag Monitored Individuals defined on the Data Source if they are found in headers inside the document. Monitored Individuals are recognized by identifier and all secondary identifiers. 

>  **NOTE:** By default, Monitored Individual discovery ignores case in the domain portion of the email address but not the name portion. For example, John.DOE@URL.COM will match John.DOE@url.com, but not john.doe@url.com.
>
>  To ignore case in the entire email address during Monitored Individual discovery, use the `Discover Monitored Individuals Ignores Case` setting. For example, John.DOE@URL.COM  will match always John.DOE@url.com, but only match john.doe@url.com if Discover Monitored Individuals Ignores Case is set to true

![image-20191217151807534](media/user_documentation/image-20191217151807534.png)

### Monitored Individual Discovery On Globanet Data Sources

Globanet's EWS Data Source only looks for Monitored Individuals in the `X-UserMailbox` header of an email. This header is provided by Globanet and typically contains exactly one Monitored Individual.

### Monitored Individual Discovery On Other Data Sources

All other data sources discover Monitored Individuals based on the `FROM`, `TO`, `CC`, and `BCC` headers. Any Monitored Individual on the Data Source with an identifier (primary or secondary) contained in any of these headers will be associated with the document.

### Supported File Formats

Discovery of monitored individuals is based on finding the email addresses of monitored individuals in the headers of an email file. Therefore, it will only work properly on .eml, .msg, and .rsmf (Relativity Short Message Format) files. Any other file format is not currently supported.

Setup
=====

The `Setup` tab aggregates the most important information about configuration, health and overall status of Trace for a workspace. It shows the currently installed version,  active tasks and their configuration, and a snapshot of instance infrastructure that’s relevant to Trace. In addition, you can manage Logging and License configuration and run a built-in self-test (`BIST`) to verify basic flow.

![1571087647200](media/user_documentation/1571087647200.png)

If an update to a new version of Trace is in progress, the Setup page will show a large red bar indicating an Update is in progress:

![1571088232926](media/user_documentation/1571088232926.png)

Tasks 
------

Tasks are ongoing background processes that are triggered by agents and run on the agent servers. They each have Run Intervals and configurations can be adjusted at a workspace level.

Each task is designed to be auto-recoverable and self-healing. For example, if there are temporary network connection issues that prevent `Data Retrieval` task from retrieving data, Trace will keep trying until network issues are resolved. No manual intervention is needed.

- **Data Retrieval:** Responsible for pulling data for Data Sources

-   **Ingestion:** Responsible for triggering import of the Data Batches into
    Relativity (part of Proactive Ingestion Framework). Any Data Transformations
    configured for the corresponding Data Source will be performed prior to ingestion.
    
-   **Data Validation:** Responsible for updating statuses of the Data Batches
    (part of Proactive Ingestion Framework)
    
- **Indexing:** Responsible for indexing data needed for searching

-   **Term Searching:** Responsible for executing searching of the Terms for
    Rule evaluations
    
-   **Rule Evaluation:** Responsible for evaluating configured Rules within the
    workspace 
    
    > **NOTE:** The Rule Evaluation task queues up work via the Service Bus framework if the Data Disposal action is in use. Trace supports any queueing framework supported by Relativity. Data Disposal  tasks are performed by the Trace Worker Agent. Additional Trace Worker Agents can be added to increase capacity. For more information, contact support@relativity.com.`
    
-   **Reporting**: Responsible for reporting on the state of the system via
    email
    
- **Data Enrichment:** Responsible for extracting nested files (attachments, contents of zip files), generating extracted text and preparing the load file that is ready for import process

  > **NOTE:** The Data Enrichment task queues up work via the Service Bus framework. Trace supports any queueing framework supported by Relativity. Enrichment tasks are performed by the `Trace Worker Agent`. Additional Trace Worker Agents can be added to increase capacity. For more information, contact support@relativity.com.

Alerts and Notifications
------------------------

By default the Reporting task will send out the system health report to the
configured instance email address every 24 hours. The defaults can be overridden
under Task Configuration on the Reporting task page. See below sample
configuration for example.

![](media/23b1404cbe20203f82f17154a8685bf1.png)

-   **Recipients:** List of emails to send the report to, separated by
    semi-colons (;) (the token \<\<EMAIL_TO_INSTANCE_SETTING\>\> will be
    replaced with the configured email address in Instance Setting: “EmailTo”
    under “kCura.Notification” section)

-   **Include Details:** Flag to determine if you want to see the details in the
    email report (default: false)

-   **Frequency In Minutes:** How often should an email report be sent out
    (default: 24hrs)

-   **Email From:** Email address to send the report from (the token
    \<\<EMAIL_FROM_INSTANCE_SETTING\>\> will be replaced with the configured
    email address in Instance Setting: `EmailTo` under `kCura.Notification`
    section)

> **NOTE:** It is required to fill in the `kCura.Notification` instance settings
`SMTPUserName`, `SMTPPassword`, `SMTPServer`, `SMTPPort` and `SMTPSSLisRequired` with
details of a functioning email delivery system in order to receive important
notifications and alerts from Relativity and Trace.
![](media/f1d0a0d1fda68815093c96764927b0df.png)

Errors and Logging
------------------

You can adjust the logging level to get more information about the system
performance specific to Trace. The Default logging level is Error. The management of the
Logging infrastructure can be adjusted via the UI console button “Manage Logs”. In
order to adjust the logging level use the “Update Trace Log Level” option. In order
to collect and display logging data use the “Trace Logs” option. You can export the
logs to a csv file with a mass operation “Export to File” at the bottom of the
list.
![](media/8373e739309804e21560cad5d48100e8.png)
![](media/9c4b600add345fd8c2200544796ac735.png)
![](media/187cb16f17210c7e4105f4df34955731.png)

> **CAUTION:** The more verbose logging levels (information/debug) can place substantial load on infrastructure in terms of number of writes and disk space usage (particularly if logs are being written to the EDDSLogging database in SQL, which is the default configuration in new Relativity instances). Don’t forget to adjust your logging level back up to Warning or Error once low level information is no longer needed.

# Analytics Automation

Relativity Trace is capable of automating builds of Conceptual Analytics Indexes, Classification Analytics Indexes, and Structured Analytics Sets. Note that the Analytics application must be installed into the workspace before analytics automation can be used.

### Conceptual and Classification Analytics

Relativity Trace will create the Trace Conceptual Analytics Index and the Trace Classification Analytics Index after install or upgrade of the Trace application if the Analytics application is installed in the workspace. By default, these indexes will not build automatically. To begin automation of an index, first perform a Full Build manually through the Relativity UI. Subsequently, Relativity Trace will automate incremental builds of the index based on the value of the `Global Analytics Build Frequency In Minutes` setting defined on the Indexing Task. To disable automatic builds of the Trace Conceptual and Classification indexes, set the value of the `Global Analytics Build Frequency In Minutes` setting to `-1`.

### Structured Analytics Sets

Relativity Trace can trigger automatic builds of any Structured Analytics Set defined in the workspace. It is possible to configure automation of multiple Structured Analytics Sets at the same time with different settings for build frequency, population scope and analysis scope.

1. Create the Structured Analytics Set(s) that will be automated and run Full Builds on them.

   > For recommendations on how to configure each of the different types of Structured Analytics Sets including Saved Search details, how frequently to run, and what kind of builds to automate, please contact support@relativity.com

2. Edit the Indexing Task from the Setup page. Under Task Settings, the `Sas Automation Configuration Json` field should automatically populate with a JSON node for every Structured Analytics Set defined in the workspace: ![image-20191223171511007](media/user_documentation/image-20191223171511007.png)

3. For each Structured Analytics Set that should be automated, perform the following steps:

   1. Find the Structured Analytics Set by looking for its name in the `SasName` field

   2. In the same JSON node (wrapped with {}), change the `Enabled` property to `true`

   3. Change the `BuildFrequencyInMinutes` property to the appropriate build frequency in minutes 

      > **NOTE:** this is the most frequently the Structured Analytics Set will be built, if a build takes longer than the interval then the next build will start when the previous one ends. Be careful not to build more frequently than needed as every build consumes resources on the Analytics server!

   4. Change the `PopulateAll` property to `true` if the underlying index should be repopulated with all relevant documents before each build (`true` for a full build, `false` for an incremental build)

      > **NOTE:** Setting `PopulateAll` to `true` can cause builds to take much longer and consume a lot more resources on the Analytics server!

   5. Change the `AnalyzeAll` property to `true` if the entire index should be analyzed in each build (`true` for a full build, `false` for an incremental build)

      > **NOTE:** Setting `AnalyzeAll` to `true` can cause builds to take much longer and consume a lot more resources on the Analytics server!

4. Click Save and the Trace Manager Agent will automate for every Structured Analytics set with Enabled = true.

Built-In Self-Test (BIST)
=========================

Built-In Self-Test (BIST) is a separate Trace Task that can be enabled in certain workspaces. By default, BIST does a basic “happy path” test of the majority of Trace functionality to make sure Trace is functioning properly. For more details including instructions on how to enable BIST in a workspace, please refer to the separate [Relativity Trace Automated Tests (BIST)](https://relativitydev.github.io/relativity-trace-documentation/bist_smoke_tests), which describes all of the different tests users can run to ensure that Relativity and Trace have been set up and configured properly.

Reporting
=========

The `Trace`->`Reports` tab serves as a place for reporting capabilities. More reports will be added to this section in the future.

Trace Terms Report
------------------
![](media/316284f452e265e8db7521909b4c00b0.png)

The Trace Terms Report provides distinct counts on how many documents matched per Rule per Term. In other words, you can quickly see current state of your Rules and associated terms.

- **Document Date Field** - the date field used to determine if a document is in the specified date range for the report
- **Date Begin** - the beginning of the date range (at 12:00AM in the Time Zone used by the selected Date Field) to use for documents in the report
- **Date End** - the ending of the date range (until 11:59PM in the Time Zone used by the selected Date Field) to use for documents in the report

# Trace and Azure Information Protection

Trace Data Sources can be configured to decrypt documents that are protected by Azure Information Protection. The Azure Information Protection Instance must have specific configuration applied in order for Trace to decrypt and read AIP protected documents. 

This configuration requires the following access / permissions:

- Administrative access to the Azure Information Protection Instance
- Access to the Azure Portal with permissions to create Application Registrations

## Configuring Azure Information Protection Instance

#### Enable Unified Labeling

It is required that the Azure Information Protection Instance being used is backed by the unified labeling platform. To see if your instance is backed by the unified labeling platform or to migrate your label to the unified labeling platform, follow these instructions:  https://docs.microsoft.com/en-us/azure/information-protection/configure-policy-migrate-labels 

#### Configure Super User

In order for Trace to be able to read AIP protected documents, the Trace Data Source username and password must be the username and password associated with a Super User in your AIP Service. You can enable Super Users and set permissions by following this guide :  https://docs.microsoft.com/en-us/azure/information-protection/configure-super-users.

#### Add an Application Registration for Super User

In order for Trace to interact with the Azure Information Protection API, you will need to create an Application Registration in the Azure Active Directory instance associated with your AIP instance. 

You can create and Application Registration using the Azure portal and following these instructions :  https://docs.microsoft.com/en-us/azure/active-directory/develop/quickstart-register-app#register-a-new-application-using-the-azure-portal.

The following parameters must be set while creating an application registration:

- **Name** : This is the name of your app registration and can be anything. Trace does not rely on the name to identify the app registration.
- **Account Type** : This should be set to "Accounts in this organizational directory only". This setting requires that the Super User configured is in this Azure Active Directory instance.
- **Redirect URI** : This should be set to "Public client". You can leave the Redirect URI as the default recommended value.

After creating the App Registration, you will need to add permissions to the registration so that it can consume the AIP API. Under the "API Permissions" tab, add the following permissions by clicking "Add a Permission" :

- Azure Rights Management Services
  - Add Delegated permissions and make sure the "user_impersonation" box is checked.
  - Click "Add Permissions"
- Microsoft Information Protection Sync Service
  - Add Delegated permissions and make sure the "UnifiedPolicy.User.Read" box is checked under the "UnifiedPolicy" tab.
  - Click "Add Permissions"

After adding the API permissions, click "Grant admin consent" on the API permissions page.

## Configuring Trace for Azure Information Protection

After finishing the configuration of the Azure Information Protection instance, you're ready to start configuring Trace for consuming AIP protected data. AIP is configured at the Data Source level. Each Data Source with AIP protected documents will need to have the following properties populated:

- **Username**: The username for the Super User of the AIP instance.

  > **NOTE:** For the Exchange Data Source Type, this username is also used to access the O365 mailbox, so the Super User configured must also have permissions to access the mailboxes being monitored

- **Password**: The password for the Super User of the AIP instance.

- **AIP Application Id**: This is the application ID of the app registration that was set up in the previous section. To find the application ID, go to the App Registration in the Azure portal and find the application ID labeled **Application (client) ID**.

- **AIP Tenant Id**: This is the Directory ID of the App Registration that was set up in the previous section. To find the tenant ID, go to the App Registration in the Azure portal and find the tenant ID labeled **Directory (tenant) ID**

  > **NOTE:** AIP will be enabled on the Data Source only if AIP Application Id *and* AIP Tenant Id are populated (not empty) on the Data Source settings. 

Considerations
==============

Usability Considerations
------------------------


-   Once a document is associated with a Rule, it will never be disassociated unless there are document updates to extracted text or metadata. The `Trace Document Retry` (mass operation) procedure will also reset the associations automatically.
-   Every Data Source has capability to be `Reset` via console buttons. Once a data source is reset,
    Trace will pull all available data again, beginning with the Start Date defined on the data source (if the Start Date is relevant to the data source type). **Depending on import profile settings, this could duplicate data in the Workspace.**
-   Task processes (Indexing, Term Searching, Rule Evaluation, etc) run simultaneously (in parallel). It may take several task cycles (based on configured `Run Interval` for each task) for the end-to-end workflow to complete fully.
-   Deleting a Trace Rule does not delete any of the corresponding Relativity infrastructure and objects that were created (i.e. dtSearch Index, Saved Searches, Batch Sets). 
-   The global dtSearch index `Trace Search Index` (created by Trace application during installation) is supported for ad-hoc searching and will be incrementally built as part of Indexing task. No other dtSearch indexes will incrementally build automatically.

## 	General Infrastructure and Environment Considerations

| **Tasks:**                       | **Ingestion**                                                                          | **Ingestion**      |    **Running Rules**                                                      |   **Running Rules**                                                                   |
|----------------------------------|----------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------|----------------------------------------------------------------------|
|                                  | **`Ingestion`, `Data Validation`, `Data Enrichment` and `Data Retrieval`** | **`Indexing`**                                                                                                                                                                                        | **`Term Searching`**                                       | **`Rule Evaluation`**                                                  |
| Summary                          | Powers the Proactive Ingestion Framework                                               | Incrementally builds Trace Search Index and Temporary Search Indexes with batches of documents Optionally: Incrementally builds and runs Analytics Jobs (Conceptual, Classification and Structured) | Searches and tags Terms in workspace on an ongoing basis | Rule will run and tag all matching documents and perform actions     |
| Task Operations                  | *`Ingestion Task`*<br>-Looks for batches that need to be imported and kicks off import<br>-Creates RIP job per batch<br>*`Data Validation Task`*<br>-Updates Data Batch status<br>*`Data Retrieval Task`*<br>-Pulls data for enabled Data Sources <br> `Data Enrichment Task`<br>-extracts/expands/enriches source native document and prepares import load file (executed by `Trace Worker Agent`) | *`Indexing Task`*<br>-Kicks off dtSearch incremental build<br>-Kicks off temporary (internal) dtSearch index builds for Term evaluation<br>-Kicks off Analytics Jobs (if configured)<br>**NOTE:** Run Interval controls frequency of temp indexes creation ONLY. Global dtSearch and Analytics Indexes are built every 60 minutes by default.                                                                                                                                                                                   | *`Term Searching Task`*<br>-Runs (searches and tags) Terms in the workspace<br>**NOTE:** Each Term Searching run interval will search up to 5 available document batches (default 10,000 documents per batch). These settings are configurable on Term Searching task.                                    | *`Rule Evaluation Task`*<br>-Evaluates each rule in the workspaces and triggers configured actions                                              |
| Recommended Run Interval         | `60` seconds                                                                             | `300` seconds                                                                                                                                                                                         | `300` seconds                                              | `300` seconds                                                          |
| Considerations and System Impact | -Speed of ingestion is determined by number of Integration Points Agents (max of 4 per instance) | -Ongoing index builds use shared instance queue, agents and Fileshare across multiple workspaces (resource pool)<br>-Every incremental build makes the old build obsolete - cleaned up by Case Manager nightly                                                                                  |                                                          | -Very complex/nested underlying Saved Searches can affect performance<br>-Saved Searches that return many documents can affect performance<br>-Many rules being evaluated often can put pressure on SQL server |

> **NOTE:** The Recommended Run Interval for the **`Reporting Task`** is 300 seconds.

Large Workspaces Infrastructure and Environment Considerations
---------------------------------------------

> **NOTE:** Use these additional recommendations to tune your environment for workspaces housing more than `10 Million` documents.


| Recommendation                                               | Explanation                                                  | Additional Notes                                             |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Separate dedicated infrastructure for `Web`, `Agent`, `SQL server` and Fileshare | Trace relies on Relativity infrastructure. In cases of shared resources (`Agent Servers`, `Fileshares`, `SQL queues`) it is recommended to dedicate separate infrastructure to limit effect on other workspaces within instance. | It is recommended to include these as a separate Resource Pool if deploying in existing environment |
| Limit Trace Task `Run Intervals` \>=`5 minutes` (300 seconds) | Each Task generates audits, SQL queries executions, creates tables, etc. and can put unnecessary pressure on the system when run too frequently | End-to-end workflow will require multiple Run Intervals for the cycle to complete |
| Limit maximum number of Rules: `50` per workspace, `500` per instance | The design of Rule Evaluation Task limits concurrent execution of multiple rules. This effect is compounded by running multiple workspaces with Rules concurrently. | Future iterations will remove this limitation                |
| Update dtSearch sub-index size to be `500K` OR `1M` (Advanced Settings) | Default sub-index size is `250K` docs, when you have more than `10` sub-indexes searches can become slow because of the number of sub-indexes<br>![1564672221173](media/1564672221173.png)<br><br>![1564672354757](media/1564672354757.png) | General rule is to keep number of sub-indexes under `10` total |
| Ensure [Data Grid for Audit](https://help.relativity.com/RelativityOne/Content/Relativity/Data_Grid/Data_Grid_for_Audit.htm) is setup OR use [Database Partitioning for Archiving Audit Records](https://community.relativity.com/s/contentdocument/06950000002JezyAAC) | Audits are generated frequently, outside of RelativityOne, they are stored in SQL which at scale creates very large tables.  It's best practice to set up table partitioning for your audits, or to deploy Data Grid solution | Data Grid for Audit is used natively in RelativityOne        |

## License and Billing Data 

Trace requires a valid license to operate. An invalid or expired license will cause Trace to enter a locked state wherein no tasks will be executed. In addition, each Trace license contains information on how billing data will be collected. Billing data is by default reported daily. Failure to report billing data for 7 days or more will also result in Trace entering a locked state.

Glossary
========

-   **Action:** a customized activity that happens when a rule is triggered

-   **Action Type:** different activities have different Action Types which can
    be customized into Actions that are associated with a Rule

-   **Agent:** process manager that runs in the background of Relativity to
    complete tasks

-   **Data Source:** setup and settings needed for a particular data source
    (e.g. Microsoft Exchange, Bloomberg, Office 365, Lync, etc.…)

-   **Data Batch:** a set of data (typically a load file) that is generated by a
    data processor as part of the Proactive Ingestion Framework

-   **Data Transformation:** an operation that can be performed on a Data Batch
    during ingestion that alters the ingested documents in some way
    (Deduplication or Replace)

-   **Monitored Individual:** a person of interest whose data is being ingested
    into Trace for monitoring

-   **Rule:** a preconfigured set of triggers that run on a regular basis and
    kick off actions

-   **Task:** an ongoing background process that is triggered by an agent

-   **Term:** search (dtSearch) condition that can be applied to Rule evaluation
    to further cull down results

Appendix A: Trace Object Architecture
=====================================

![](media/c961c7a2692a2b4f30a91566d902a2f6.png)

Appendix B: Trace Document Extraction Fields
============================================

Trace automatically extracts metadata information for Microsoft Office 365 Data Source and Relativity Data Extraction Data Source. Below is the list of all the supported fields. There are four major categories for fields:

-   `Email` Only – fields that apply to emails only

-   `Email` and `Document` – fields that apply to emails and other documents (Word,
    Excel, PowerPoint, etc...)

-   `Document` Only – fields that apply to documents but not to emails

-   `Calculated` – fields that are calculated dynamically by Trace

| **Trace Field Category** | **Relativity Field**          | **Field Type**    | **Field Description**                                        |
| ------------------------ | ----------------------------- | ----------------- | ------------------------------------------------------------ |
| Email Only               | Attachment List               | Long Text         | Attachment file names of all child items in a family group, delimited by semicolon, only present on parent items. |
| Email Only               | BCC                           | Long Text         | The name(s) (when available) and email address(es) of the Blind Carbon Copy recipient(s) of an email message. |
| Email Only               | Can Forward                   | Yes/No            | The indicator whether the mail message can be forwarded.     |
| Email Only               | CC                            | Long Text         | The name(s) (when available) and email address(es) of the Carbon Copy recipient(s) of an email message. |
| Email Only               | Conversation Index            | Long Text         | Email thread created by the email system. This is a 44-character string of numbers and letters that is created in the initial email and has 10 characters added for each reply or forward of an email. |
| Email Only               | Conversation                  | Long Text         | Normalized subject of email messages. This is the subject line of the email after removing the RE and FW that are added by the system when emails are forwarded or replied to. |
| Email Only               | Creator Email Entry ID        | Long Text         | The unique Identifier of creator an email in an email store  |
| Email Only               | Delivery Receipt Requested    | Yes/No            | The yes/no indicator of whether a delivery receipt was requested for an e-mail. |
| Email Only               | Email Categories              | Long Text         | Category(ies) assigned to an email message.                  |
| Email Only               | Email Created Date/Time       | Date              | The date and time at which an email was created (converted to UTC). |
| Email Only               | Email Format                  | Single Choice     | The indicator of whether an email is HTML, Rich Text, or Plain Text. |
| Email Only               | Email In Reply To ID          | Long Text         | The internal metadata value within an email for the reply to ID. |
| Email Only               | Email Last Modified Date/Time | Date              | Date and time that the email message last modified (converted to UTC). |
| Email Only               | Email Sensitivity             | Single Choice     | The indicator set on an email to denote the email's level of privacy. |
| Email Only               | From Address                  | Long Text         | The e-mail address for the messaging user represented by the sender. |
| Email Only               | From                          | Fixed-Length Text | The name (when available) and email address of the sender of an email message. |
| Email Only               | From                          | Date              | Date and time that the email message was sent (converted to UTC). |
| Email Only               | Importance                    | Single Choice     | Notation created for email messages to note a higher level of importance than other email messages added by the email originator. |
| Email Only               | Last Modifier Email Entry ID  | Long Text         | The unique Identifier of last modifier of an email in an mail store |
| Email Only               | Message Class                 | Single Choice     | A single choice field that can be one of: Email, Edoc, or Attach. |
| Email Only               | Message ID                    | Fixed-Length Text | The message number created by an email application and extracted from the email's metadata. |
| Email Only               | Priority                      | Single Choice     | The priority of the email message.                           |
| Email Only               | Read Receipt Requested        | Yes/No            | The yes/no indicator of whether a read receipt was requested for an e-mail. |
| Email Only               | Received By Email Entry ID    | Long Text         | The unique Identifier of an email in an mail store           |
| Email Only               | Received Date/Time            | Date              | Date and time that the email message was received (converted to UTC). |
| Email Only               | Sender Email Entry ID         | Long Text         | The unique Identifier of an email in an mail store           |
| Email Only               | Subject                       | Long Text         | Subject of the email message.                                |
| Email Only               | To                            | Long Text         | The name(s) (when available) and email address(es) of the recipient(s) of an email message. |
| Email and Document       | Author                        | Fixed-Length Text | Original composer of document or sender of email message.    |
| Email and Document       | Created Date/Time             | Date              | Date and time from the Date Created property extracted from the original file or email message (UTC). |
| Email and Document       | Extracted Text                | Long Text         | Complete text extracted from content of electronic files or OCR data field. This field holds the hidden comments of MS Office files. |
| Document Only            | Comments                      | Long Text         | Comments extracted from the metadata of the native file.     |
| Document Only            | Company                       | Fixed-Length Text | The internal value entered for the company associated with a Microsoft Office document. |
| Document Only            | Document Subject              | Long Text         | Subject of the document extracted from the properties of the native file. |
| Document Only            | Document Title                | Long Text         | The title of a non-email document. This is blank if there is no value available. |
| Document Only            | Keywords                      | Long Text         | The internal value entered for keywords associated with a Microsoft Office document. |
| Document Only            | Last Printed Date/Time        | Date              | Date and time that the document was last printed.            |
| Document Only            | Last Saved By                 | Fixed-Length Text | The internal value indicating the last user to save a document. |
| Document Only            | Last Saved Date/Time          | Date              | The internal value entered for the date and time at which a document was last saved. |
| Calculated               | Container ID                  | Fixed-Length Text | Unique identifier of the container file in which the document originated. This is used to identify or group files that came from the same container. |
| Calculated               | Container Name                | Fixed-Length Text | Name of the container file in which the document originated. |
| Calculated               | Control Number                | Fixed-Length Text | The identifier of the document.                              |
| Calculated               | Email Has Attachments         | Yes/No            | The yes/no indicator of whether an email has children (attachments). |
| Calculated               | Email Store Name              | Fixed-Length Text | Any email, contact, appointment, etc. that is extracted from an email container (PST, OST, NSF, MBOX, etc) will have this field populated with the name of that email container. |
| Calculated               | Extracted Text Size in KB     | Decimal           | Indicates the size of the extracted text field in kilobytes  |
| Calculated               | Family Group                  | Fixed-Length Text | Group the file belongs to (used to identify the group if attachment fields are not used). Formerly "Group Identifier" |
| Calculated               | File Extension                | Fixed-Length Text | The extension of the file, as assigned by the processing engine after it reads the header information from the original file |
| Calculated               | File Name                     | Fixed-Length Text | The original name of the file                                |
| Calculated               | File Size                     | Decimal           | **DO NOT MAP** this field in integration point profile. A value for this field is automatically calculated by Relativity on import.                                                                                             |
| Calculated               | File Type                     | Fixed-Length Text | Description that represents the file type to the Windows Operating System |
| Calculated               | Native File                   | Long Text         | The path to a copy of a file for loading into Relativity.    |
| Calculated               | Number of Attachments         | Decimal           | Number of files attached to a parent document.               |
| Calculated               | Other Metadata                | Long Text         | Metadata extracted during processing for additional fields beyond the list of processing fields available for mapping |
| Calculated               | Parent Document ID            | Fixed-Length Text | Document ID (Control Number) of the parent document. Empty for top level (original native) documents. For multiple levels of descendants, this field will always be populated with the Document ID (Control Number) of the top level (original native) document for every descendant document. |
| Calculated               | Password Protected            | Single Choice     | Indicates the documents that were password protected. It contains the value Decrypted if the password was identified, Encrypted if the password was not identified, or no value if the file was not password protected. |
| Calculated               | Recipient Count               | Decimal           | The total count of recipients in an email which includes the To, CC, and BCC fields |
| Calculated               | Trace Data Transformations    | Multiple Object   | Data Transformations that have been applied to the document  |
| Calculated               | Trace Document Hash           | Fixed-Length Text | Calculated hash value that is used to determine if a document is a duplicate of another document |
| Calculated               | Trace Error Details           | Long Text         | Details of errors encountered during the extraction/expansion |
| Calculated               | Trace Has Errors              | Yes/No            | Indicates if any errors occurred during extraction/expansion |
| Calculated               | Trace Monitored Individuals   | Multiple Object   | Monitored individuals associated with Data Source (used for retrieval and billing) |

Appendix C: Create Email Fields Map Integration Point Profile
=============================================================

> **NOTE:** Make sure the Integration Points application is installed in this
Workspace before proceeding.

> **NOTE:** Load File Templates are generated by Trace Agent **ONLY** in
workspaces that are enabled for “Continuous” run option in “Setup” tab. Ensure `Data Retrieval` task runs **at least once before configuring** Integration Point Profile

> **NOTE:** Trace uses default Relativity fields for email and attachment data that ship with `Create & Map Field Catalog – Full` (v0.0.1.5+). **Trace recommends use of this application, however it is totally optional and one can choose to create custom fields or re-use fields from an existing template.** You can install it to the target workspace like any other application which will bring in all
the needed fields with standardized names that are easily auto-mapped via “Map
Fields” integration point profile feature. If you don’t see this application in
your library, reach out to `support@relativity.com`.
![](media/045f66f33011aa62ff7d00b2e05274c2.png)

## Setup Integration Point Profile

1. Go to the Integration Points: Integration Point Profile tab

2. Click “New Integration Point Profile”

3. On Page 1 (Setup):

   1.  Enter a name for the Profile “Email Fields Map Profile”

   2.  Type = “Import”

   3.  Source = “Load File”

   4.  Keep all other settings as Default and click “Next”

4. On Page 2 (Connect to Source):

   1.  Fill in preferred Workspace Destination Folder

   2.  Import Source: Select the Load File Template in the `LoadFileTemplates`
       Folder that matches the Data Source type that will use this Integration
       Point Profile (example: Microsoft Exchange data source):

       ![](media/f13604a6dda3fdda62e890f80b332b1e.png)

   3.  Change the Newline character to (`ASCII: 010`)

   4.  Keep all other settings as default and press `Next`

5. On Page 3, map all the fields you need from the Load File Template to your
   Workspace fields and configure Settings.

   1.  Click the `Map Fields` button to map all fields in your Workspace that
       have identical names to fields from the Load File Template

   2.  Next, double-click `Control Number` in the Source (far left) list and then
       double-click Control Number (or custom Identifier Field) in the Destination (far right) list to map the Control Number field.
   
3.  Map any other unmapped fields in the Source list to the appropriate
       field in the Destination list. It is not required to map every field.
       For any missing fields in your Workspace, create them in the Workspace
       Administration: Fields Tab. See [Appendix B: Load File Fields for Emails and Attachments](#appendix-b-trace-document-extraction-fields) for a list and associated definition for all Trace
       fields.
    
4.  At the bottom, configure Settings to match the screenshot below
   
    ![](media/2fb725ce41fc7828621e86298ecd03c4.png)
   
5.  Press “Save”

> **NOTE:** You can edit the Integration Point Profile settings at any time, however existing completed Data Batches will not automatically re-import the data with new settings/mappings.
